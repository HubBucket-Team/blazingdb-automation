{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import dask_xgboost as dxgb_gpu\n",
    "# import dask\n",
    "# import dask_cudf\n",
    "# from dask.delayed import delayed\n",
    "# from dask.distributed import Client, wait\n",
    "import xgboost as xgb\n",
    "import cudf\n",
    "from cudf.dataframe import DataFrame\n",
    "from collections import OrderedDict\n",
    "import gc\n",
    "from glob import glob\n",
    "import os\n",
    "import pyblazing\n",
    "import pandas as pd\n",
    "import time\n",
    "from chronometer import Chronometer\n",
    "\n",
    "from pyblazing import FileSystemType, SchemaFrom, DriverType\n",
    "\n",
    "def initialize_rmm_pool():\n",
    "    from librmm_cffi import librmm_config as rmm_cfg\n",
    "\n",
    "    rmm_cfg.use_pool_allocator = True\n",
    "    # rmm_cfg.initial_pool_size = 2<<30 # set to 2GiB. Default is 1/2 total GPU memory\n",
    "\n",
    "    rmm_cfg.initial_pool_size = 2 << 31\n",
    "\n",
    "    import cudf\n",
    "    return cudf._gdf.rmm_initialize()\n",
    "\n",
    "\n",
    "def initialize_rmm_no_pool():\n",
    "    from librmm_cffi import librmm_config as rmm_cfg\n",
    "\n",
    "    rmm_cfg.use_pool_allocator = False\n",
    "    import cudf\n",
    "    return cudf._gdf.rmm_initialize()\n",
    "\n",
    "\n",
    "def register_hdfs():\n",
    "    print('*** Register a HDFS File System ***')\n",
    "    fs_status = pyblazing.register_file_system(\n",
    "        authority=\"35.203.181.221\",\n",
    "        type=FileSystemType.HDFS,\n",
    "        root=\"/\",\n",
    "        params={\n",
    "            \"host\": \"35.203.181.221\",\n",
    "            \"port\": 54310,\n",
    "            \"user\": \"hadoop\",\n",
    "            \"driverType\": DriverType.LIBHDFS3,\n",
    "            \"kerberosTicket\": \"\"\n",
    "        }\n",
    "    )\n",
    "    print(fs_status)\n",
    "\n",
    "\n",
    "def deregister_hdfs():\n",
    "    fs_status = pyblazing.deregister_file_system(authority=\"35.203.181.221\")\n",
    "    print(fs_status)\n",
    "\n",
    "def register_posix():\n",
    "    print('*** Register a POSIX File System ***')\n",
    "    fs_status = pyblazing.register_file_system(\n",
    "        authority=\"mortgage\",\n",
    "        type=FileSystemType.POSIX,\n",
    "        root=\"/\"\n",
    "    )\n",
    "    print(fs_status)\n",
    "\n",
    "def deregister_posix():\n",
    "    fs_status = pyblazing.deregister_file_system(authority=\"mortgage\")\n",
    "    print(fs_status)\n",
    "\n",
    "from libgdf_cffi import ffi, libgdf\n",
    "\n",
    "def get_dtype_values(dtypes):\n",
    "    values = []\n",
    "    def gdf_type(type_name):\n",
    "        dicc = {\n",
    "            'str': libgdf.GDF_STRING,\n",
    "            'date': libgdf.GDF_DATE64,\n",
    "            'date64': libgdf.GDF_DATE64,\n",
    "            'date32': libgdf.GDF_DATE32,\n",
    "            'timestamp': libgdf.GDF_TIMESTAMP,\n",
    "            'category': libgdf.GDF_CATEGORY,\n",
    "            'float': libgdf.GDF_FLOAT32,\n",
    "            'double': libgdf.GDF_FLOAT64,\n",
    "            'float32': libgdf.GDF_FLOAT32,\n",
    "            'float64': libgdf.GDF_FLOAT64,\n",
    "            'short': libgdf.GDF_INT16,\n",
    "            'long': libgdf.GDF_INT64,\n",
    "            'int': libgdf.GDF_INT32,\n",
    "            'int32': libgdf.GDF_INT32,\n",
    "            'int64': libgdf.GDF_INT64,\n",
    "        }\n",
    "        if dicc.get(type_name):\n",
    "            return dicc[type_name]\n",
    "        return libgdf.GDF_INT64\n",
    "\n",
    "    for key in dtypes:\n",
    "        values.append( gdf_type(dtypes[key]))\n",
    "\n",
    "    print('>>>> dtyps for', dtypes.values())\n",
    "    print(values)\n",
    "    return values\n",
    "\n",
    "def get_type_schema(path):\n",
    "    format = path.split('.')[-1]\n",
    "\n",
    "    if format == 'parquet':\n",
    "        return SchemaFrom.ParquetFile\n",
    "    elif format == 'csv' or format == 'psv' or format.startswith(\"txt\"):\n",
    "        return SchemaFrom.CsvFile\n",
    "\n",
    "def null_workaround(df, **kwargs):\n",
    "    for column, data_type in df.dtypes.items():\n",
    "        if str(data_type) == \"category\":\n",
    "            df[column] = df[column].astype('int32').fillna(-1)\n",
    "        if str(data_type) in ['int8', 'int16', 'int32', 'int64', 'float32', 'float64']:\n",
    "            df[column] = df[column].fillna(-1)\n",
    "    return df\n",
    "\n",
    "def open_perf_table(table_ref):\n",
    "    for key in table_ref.keys():\n",
    "        sql = 'select * from main.%(table_name)s' % {\"table_name\": key.table_name}\n",
    "        return pyblazing.run_query(sql, table_ref)\n",
    "\n",
    "def run_gpu_workflow(quarter=1, year=2000, perf_file=\"\", **kwargs):\n",
    "\n",
    "    import time\n",
    "\n",
    "    load_start_time = time.time()\n",
    "\n",
    "    if use_hdfs:\n",
    "        register_hdfs()\n",
    "    else:\n",
    "        register_posix()\n",
    "\n",
    "    names = gpu_load_names()\n",
    "    acq_gdf = gpu_load_acquisition_csv(acquisition_path=acq_data_path + \"/Acquisition_\"\n",
    "                                                        + str(year) + \"Q\" + str(quarter) + \".txt\")\n",
    "\n",
    "    gdf = gpu_load_performance_csv(perf_file)\n",
    "\n",
    "    load_end_time = time.time()\n",
    "\n",
    "    etl_start_time = time.time()\n",
    "    \n",
    "    acq_gdf_results = merge_names(acq_gdf, names)\n",
    "\n",
    "    everdf_results = create_ever_features(gdf)\n",
    "\n",
    "    delinq_merge_results = create_delinq_features(gdf)\n",
    "\n",
    "    new_everdf_results = join_ever_delinq_features(everdf_results.columns, delinq_merge_results.columns)\n",
    "\n",
    "    joined_df_results = create_joined_df(gdf.columns, new_everdf_results.columns)\n",
    "    del (new_everdf_results)\n",
    "\n",
    "    testdf_results = create_12_mon_features_union_alt(joined_df_results.columns)\n",
    "    # testdf_results = create_12_mon_features_union(joined_df_results.columns)\n",
    "    \n",
    "    testdf = testdf_results.columns\n",
    "    new_joined_df_results = combine_joined_12_mon(joined_df_results.columns, testdf)\n",
    "    del (testdf)\n",
    "    del (joined_df_results)\n",
    "    perf_df_results = final_performance_delinquency(gdf.columns, new_joined_df_results.columns)\n",
    "    del (gdf)\n",
    "    del (new_joined_df_results)\n",
    "\n",
    "    final_gdf_results = join_perf_acq_gdfs(perf_df_results.columns, acq_gdf_results.columns)\n",
    "    del (perf_df_results)\n",
    "    del (acq_gdf_results)\n",
    "    \n",
    "    final_gdf = last_mile_cleaning(final_gdf_results.columns)\n",
    "\n",
    "    if use_hdfs:\n",
    "        deregister_hdfs()\n",
    "    else:\n",
    "        deregister_posix()\n",
    "\n",
    "    etl_end_time = time.time()\n",
    "    \n",
    "    return [final_gdf, (load_end_time - load_start_time), (etl_end_time - etl_start_time)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gpu_load_performance_csv(performance_path, **kwargs):\n",
    "    \"\"\" Loads performance data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    GPU DataFrame\n",
    "    \"\"\"\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "\n",
    "    cols = [\n",
    "        \"loan_id\", \"monthly_reporting_period\", \"servicer\", \"interest_rate\", \"current_actual_upb\",\n",
    "        \"loan_age\", \"remaining_months_to_legal_maturity\", \"adj_remaining_months_to_maturity\",\n",
    "        \"maturity_date\", \"msa\", \"current_loan_delinquency_status\", \"mod_flag\", \"zero_balance_code\",\n",
    "        \"zero_balance_effective_date\", \"last_paid_installment_date\", \"foreclosed_after\",\n",
    "        \"disposition_date\", \"foreclosure_costs\", \"prop_preservation_and_repair_costs\",\n",
    "        \"asset_recovery_costs\", \"misc_holding_expenses\", \"holding_taxes\", \"net_sale_proceeds\",\n",
    "        \"credit_enhancement_proceeds\", \"repurchase_make_whole_proceeds\", \"other_foreclosure_proceeds\",\n",
    "        \"non_interest_bearing_upb\", \"principal_forgiveness_upb\", \"repurchase_make_whole_proceeds_flag\",\n",
    "        \"foreclosure_principal_write_off_amount\", \"servicing_activity_indicator\"\n",
    "    ]\n",
    "\n",
    "    dtypes = OrderedDict([\n",
    "        (\"loan_id\", \"int64\"),\n",
    "        (\"monthly_reporting_period\", \"date\"),\n",
    "        (\"servicer\", \"category\"),\n",
    "        (\"interest_rate\", \"float64\"),\n",
    "        (\"current_actual_upb\", \"float64\"),\n",
    "        (\"loan_age\", \"float64\"),\n",
    "        (\"remaining_months_to_legal_maturity\", \"float64\"),\n",
    "        (\"adj_remaining_months_to_maturity\", \"float64\"),\n",
    "        (\"maturity_date\", \"date\"),\n",
    "        (\"msa\", \"float64\"),\n",
    "        (\"current_loan_delinquency_status\", \"int32\"),\n",
    "        (\"mod_flag\", \"category\"),\n",
    "        (\"zero_balance_code\", \"category\"),\n",
    "        (\"zero_balance_effective_date\", \"date\"),\n",
    "        (\"last_paid_installment_date\", \"date\"),\n",
    "        (\"foreclosed_after\", \"date\"),\n",
    "        (\"disposition_date\", \"date\"),\n",
    "        (\"foreclosure_costs\", \"float64\"),\n",
    "        (\"prop_preservation_and_repair_costs\", \"float64\"),\n",
    "        (\"asset_recovery_costs\", \"float64\"),\n",
    "        (\"misc_holding_expenses\", \"float64\"),\n",
    "        (\"holding_taxes\", \"float64\"),\n",
    "        (\"net_sale_proceeds\", \"float64\"),\n",
    "        (\"credit_enhancement_proceeds\", \"float64\"),\n",
    "        (\"repurchase_make_whole_proceeds\", \"float64\"),\n",
    "        (\"other_foreclosure_proceeds\", \"float64\"),\n",
    "        (\"non_interest_bearing_upb\", \"float64\"),\n",
    "        (\"principal_forgiveness_upb\", \"float64\"),\n",
    "        (\"repurchase_make_whole_proceeds_flag\", \"category\"),\n",
    "        (\"foreclosure_principal_write_off_amount\", \"float64\"),\n",
    "        (\"servicing_activity_indicator\", \"category\")\n",
    "    ])\n",
    "    print(performance_path)\n",
    "    performance_table = pyblazing.create_table(table_name='perf', type=get_type_schema(performance_path), path=performance_path, delimiter='|', names=cols, dtypes=get_dtype_values(dtypes), skip_rows=1)\n",
    "    Chronometer.show(chronometer, 'Read Performance CSV')\n",
    "    return performance_table\n",
    "\n",
    "def gpu_load_acquisition_csv(acquisition_path, **kwargs):\n",
    "    \"\"\" Loads acquisition data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    GPU DataFrame\n",
    "    \"\"\"\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "\n",
    "    cols = [\n",
    "        'loan_id', 'orig_channel', 'seller_name', 'orig_interest_rate', 'orig_upb', 'orig_loan_term',\n",
    "        'orig_date', 'first_pay_date', 'orig_ltv', 'orig_cltv', 'num_borrowers', 'dti', 'borrower_credit_score',\n",
    "        'first_home_buyer', 'loan_purpose', 'property_type', 'num_units', 'occupancy_status', 'property_state',\n",
    "        'zip', 'mortgage_insurance_percent', 'product_type', 'coborrow_credit_score', 'mortgage_insurance_type',\n",
    "        'relocation_mortgage_indicator'\n",
    "    ]\n",
    "\n",
    "    dtypes = OrderedDict([\n",
    "        (\"loan_id\", \"int64\"),\n",
    "        (\"orig_channel\", \"category\"),\n",
    "        (\"seller_name\", \"category\"),\n",
    "        (\"orig_interest_rate\", \"float64\"),\n",
    "        (\"orig_upb\", \"int64\"),\n",
    "        (\"orig_loan_term\", \"int64\"),\n",
    "        (\"orig_date\", \"date\"),\n",
    "        (\"first_pay_date\", \"date\"),\n",
    "        (\"orig_ltv\", \"float64\"),\n",
    "        (\"orig_cltv\", \"float64\"),\n",
    "        (\"num_borrowers\", \"float64\"),\n",
    "        (\"dti\", \"float64\"),\n",
    "        (\"borrower_credit_score\", \"float64\"),\n",
    "        (\"first_home_buyer\", \"category\"),\n",
    "        (\"loan_purpose\", \"category\"),\n",
    "        (\"property_type\", \"category\"),\n",
    "        (\"num_units\", \"int64\"),\n",
    "        (\"occupancy_status\", \"category\"),\n",
    "        (\"property_state\", \"category\"),\n",
    "        (\"zip\", \"int64\"),\n",
    "        (\"mortgage_insurance_percent\", \"float64\"),\n",
    "        (\"product_type\", \"category\"),\n",
    "        (\"coborrow_credit_score\", \"float64\"),\n",
    "        (\"mortgage_insurance_type\", \"float64\"),\n",
    "        (\"relocation_mortgage_indicator\", \"category\")\n",
    "    ])\n",
    "\n",
    "    print(acquisition_path)\n",
    "\n",
    "    acquisition_table = pyblazing.create_table(table_name='acq', type=get_type_schema(acquisition_path), path=acquisition_path, delimiter='|', names=cols, dtypes=get_dtype_values(dtypes), skip_rows=1)\n",
    "    Chronometer.show(chronometer, 'Read Acquisition CSV')\n",
    "    return acquisition_table\n",
    "\n",
    "def gpu_load_names(**kwargs):\n",
    "    \"\"\" Loads names used for renaming the banks\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    GPU DataFrame\n",
    "    \"\"\"\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "\n",
    "    cols = [\n",
    "        'seller_name', 'new_seller_name'\n",
    "    ]\n",
    "\n",
    "    dtypes = OrderedDict([\n",
    "        (\"seller_name\", \"category\"),\n",
    "        (\"new_seller_name\", \"category\"),\n",
    "    ])\n",
    "\n",
    "    names_table = pyblazing.create_table(table_name='names', type=get_type_schema(col_names_path), path=col_names_path, delimiter='|', names=cols, dtypes=get_dtype_values(dtypes), skip_rows=1)\n",
    "    Chronometer.show(chronometer, 'Read Names CSV')\n",
    "    return names_table\n",
    "\n",
    "\n",
    "def merge_names(names_table, acq_table):\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "    tables = {names_table.name: names_table.columns,\n",
    "              acq_table.name:acq_table.columns}\n",
    "\n",
    "    query = \"\"\"SELECT loan_id, orig_channel, orig_interest_rate, orig_upb, orig_loan_term, \n",
    "        orig_date, first_pay_date, orig_ltv, orig_cltv, num_borrowers, dti, borrower_credit_score, \n",
    "        first_home_buyer, loan_purpose, property_type, num_units, occupancy_status, property_state,\n",
    "        zip, mortgage_insurance_percent, product_type, coborrow_credit_score, mortgage_insurance_type, \n",
    "        relocation_mortgage_indicator, new_seller_name as seller_name \n",
    "        FROM main.acq as a LEFT OUTER JOIN main.names as n ON  a.seller_name = n.seller_name\"\"\"\n",
    "    result = pyblazing.run_query(query, tables)\n",
    "    Chronometer.show(chronometer, 'Create Acquisition (Merge Names)')\n",
    "    return result\n",
    "\n",
    "\n",
    "def merge_names_old_rapids(acq_gdf, names):\n",
    "    acq_gdf = acq_gdf.merge(names, how='left', on=['seller_name'])\n",
    "    acq_gdf.drop_column('seller_name')\n",
    "    acq_gdf['seller_name'] = acq_gdf['new_seller_name']\n",
    "    acq_gdf.drop_column('new_seller_name')\n",
    "\n",
    "    return acq_gdf\n",
    "\n",
    "\n",
    "def create_ever_features(table, **kwargs):\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "    query = \"\"\"SELECT loan_id,\n",
    "        max(current_loan_delinquency_status) >= 1 as ever_30, \n",
    "        max(current_loan_delinquency_status) >= 3 as ever_90,\n",
    "        max(current_loan_delinquency_status) >= 6 as ever_180\n",
    "        FROM main.perf group by loan_id\"\"\"\n",
    "    result = pyblazing.run_query(query, {table.name: table.columns})\n",
    "    Chronometer.show(chronometer, 'Create Ever Features')\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_ever_features_old_rapids(gdf, **kwargs):\n",
    "    everdf = gdf[['loan_id', 'current_loan_delinquency_status']]\n",
    "    everdf = everdf.groupby('loan_id', method='hash').max()\n",
    "    del (gdf)\n",
    "    everdf['ever_30'] = (everdf['max_current_loan_delinquency_status'] >= 1).astype('int8')\n",
    "    everdf['ever_90'] = (everdf['max_current_loan_delinquency_status'] >= 3).astype('int8')\n",
    "    everdf['ever_180'] = (everdf['max_current_loan_delinquency_status'] >= 6).astype('int8')\n",
    "    everdf.drop_column('max_current_loan_delinquency_status')\n",
    "    return everdf\n",
    "\n",
    "\n",
    "def create_delinq_features(table, **kwargs):\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "    query = \"\"\"SELECT loan_id,\n",
    "        min(monthly_reporting_period) as delinquency_30\n",
    "        FROM main.perf where current_loan_delinquency_status >= 1 group by loan_id\"\"\"\n",
    "    result_delinq_30 = pyblazing.run_query(query, {table.name: table.columns})\n",
    "    \n",
    "    query = \"\"\"SELECT loan_id,\n",
    "        min(monthly_reporting_period) as delinquency_90\n",
    "        FROM main.perf where current_loan_delinquency_status >= 3 group by loan_id\"\"\"\n",
    "    result_delinq_90 = pyblazing.run_query(query, {table.name: table.columns})\n",
    "    \n",
    "    query = \"\"\"SELECT loan_id,\n",
    "        min(monthly_reporting_period) as delinquency_180\n",
    "        FROM main.perf where current_loan_delinquency_status >= 6 group by loan_id\"\"\"\n",
    "    result_delinq_180 = pyblazing.run_query(query, {table.name: table.columns})\n",
    "    \n",
    "    \n",
    "\n",
    "    new_tables = {\"delinq_30\": result_delinq_30.columns, \"delinq_90\": result_delinq_90.columns, \"delinq_180\": result_delinq_180.columns}\n",
    "    query = \"\"\"SELECT d30.loan_id, delinquency_30, COALESCE(delinquency_90, DATE '1970-01-01') as delinquency_90,\n",
    "                COALESCE(delinquency_180, DATE '1970-01-01') as delinquency_180 FROM main.delinq_30 as d30\n",
    "                LEFT OUTER JOIN main.delinq_90 as d90 ON d30.loan_id = d90.loan_id\n",
    "                LEFT OUTER JOIN main.delinq_180 as d180 ON d30.loan_id = d180.loan_id\"\"\"\n",
    "    result_merge = pyblazing.run_query(query, new_tables)\n",
    "    Chronometer.show(chronometer, 'Create deliquency features')\n",
    "    return result_merge\n",
    "\n",
    "\n",
    "def create_delinq_features_old_rapids(gdf, **kwargs):\n",
    "    delinq_gdf = gdf[['loan_id', 'monthly_reporting_period', 'current_loan_delinquency_status']]\n",
    "    del (gdf)\n",
    "    delinq_30 = delinq_gdf.query('current_loan_delinquency_status >= 1')[\n",
    "        ['loan_id', 'monthly_reporting_period']].groupby('loan_id', method='hash').min()\n",
    "    delinq_30['delinquency_30'] = delinq_30['min_monthly_reporting_period']\n",
    "    delinq_30.drop_column('min_monthly_reporting_period')\n",
    "    delinq_90 = delinq_gdf.query('current_loan_delinquency_status >= 3')[\n",
    "        ['loan_id', 'monthly_reporting_period']].groupby('loan_id', method='hash').min()\n",
    "    delinq_90['delinquency_90'] = delinq_90['min_monthly_reporting_period']\n",
    "    delinq_90.drop_column('min_monthly_reporting_period')\n",
    "    delinq_180 = delinq_gdf.query('current_loan_delinquency_status >= 6')[\n",
    "        ['loan_id', 'monthly_reporting_period']].groupby('loan_id', method='hash').min()\n",
    "    delinq_180['delinquency_180'] = delinq_180['min_monthly_reporting_period']\n",
    "    delinq_180.drop_column('min_monthly_reporting_period')\n",
    "    del (delinq_gdf)\n",
    "\n",
    "    delinq_merge = delinq_30.merge(delinq_90, how='left', on=['loan_id'], type='hash')\n",
    "    delinq_merge['delinquency_90'] = delinq_merge['delinquency_90'].fillna(\n",
    "        np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    delinq_merge = delinq_merge.merge(delinq_180, how='left', on=['loan_id'], type='hash')\n",
    "    delinq_merge['delinquency_180'] = delinq_merge['delinquency_180'].fillna(\n",
    "        np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    del (delinq_30)\n",
    "    del (delinq_90)\n",
    "    del (delinq_180)\n",
    "    return delinq_merge\n",
    "\n",
    "\n",
    "# everdf_tmp has loan_id, ever_30, ever_90, ever_180\n",
    "# delinq_merge has loan_id, delinquency_30, delinquency_90, delinquency_180\n",
    "def join_ever_delinq_features(everdf_tmp, delinq_merge, **kwargs):\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "    tables = {\"everdf\": everdf_tmp, \"delinq\": delinq_merge}\n",
    "    query = \"\"\"SELECT everdf.loan_id as loan_id, ever_30, ever_90, ever_180,\n",
    "                  COALESCE(delinquency_30, DATE '1970-01-01') as delinquency_30,\n",
    "                  COALESCE(delinquency_90, DATE '1970-01-01') as delinquency_90,\n",
    "                  COALESCE(delinquency_180, DATE '1970-01-01') as delinquency_180 FROM main.everdf as everdf\n",
    "                  LEFT OUTER JOIN main.delinq as delinq ON everdf.loan_id = delinq.loan_id\"\"\"\n",
    "    result_merge = pyblazing.run_query(query, tables)\n",
    "    Chronometer.show(chronometer, 'Create ever deliquency features')\n",
    "    return result_merge\n",
    "\n",
    "\n",
    "def join_ever_delinq_features_old_rapids(everdf_tmp, delinq_merge, **kwargs):\n",
    "    everdf = everdf_tmp.merge(delinq_merge, on=['loan_id'], how='left', type='hash')\n",
    "    del (everdf_tmp)\n",
    "    del (delinq_merge)\n",
    "    everdf['delinquency_30'] = everdf['delinquency_30'].fillna(\n",
    "        np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    everdf['delinquency_90'] = everdf['delinquency_90'].fillna(\n",
    "        np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    everdf['delinquency_180'] = everdf['delinquency_180'].fillna(\n",
    "        np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    return everdf\n",
    "\n",
    "\n",
    "def create_joined_df(gdf, everdf, **kwargs):\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "    tables = {\"perf\": gdf, \"everdf\": everdf}\n",
    "    \n",
    "    query = \"\"\"SELECT perf.loan_id as loan_id, \n",
    "                perf.monthly_reporting_period as mrp_timestamp,\n",
    "                EXTRACT(MONTH FROM perf.monthly_reporting_period) as timestamp_month,\n",
    "                EXTRACT(YEAR FROM perf.monthly_reporting_period) as timestamp_year,\n",
    "                COALESCE(perf.current_loan_delinquency_status, -1) as delinquency_12,\n",
    "                COALESCE(perf.current_actual_upb, 999999999.9) as upb_12,\n",
    "                everdf.ever_30 as ever_30,\n",
    "                everdf.ever_90 as ever_90, \n",
    "                everdf.ever_180 as ever_180, \n",
    "                COALESCE(everdf.delinquency_30, DATE '1970-01-01') as delinquency_30, \n",
    "                COALESCE(everdf.delinquency_90, DATE '1970-01-01') as delinquency_90, \n",
    "                COALESCE(everdf.delinquency_180, DATE '1970-01-01') as delinquency_180\n",
    "                FROM main.perf as perf \n",
    "                LEFT OUTER JOIN main.everdf as everdf ON perf.loan_id = everdf.loan_id\"\"\"\n",
    "\n",
    "    results = pyblazing.run_query(query, tables)\n",
    "    Chronometer.show(chronometer, 'Create Joined DF')\n",
    "    return results\n",
    "\n",
    "\n",
    "def create_joined_df_old_rapids(gdf, everdf, **kwargs):\n",
    "    test = gdf[['loan_id', 'monthly_reporting_period', 'current_loan_delinquency_status', 'current_actual_upb']]\n",
    "    del (gdf)\n",
    "    test['timestamp'] = test['monthly_reporting_period']\n",
    "    test.drop_column('monthly_reporting_period')\n",
    "    test['timestamp_month'] = test['timestamp'].dt.month\n",
    "    test['timestamp_year'] = test['timestamp'].dt.year\n",
    "    test['delinquency_12'] = test['current_loan_delinquency_status']\n",
    "    test.drop_column('current_loan_delinquency_status')\n",
    "    test['upb_12'] = test['current_actual_upb']\n",
    "    test.drop_column('current_actual_upb')\n",
    "    test['upb_12'] = test['upb_12'].fillna(999999999)\n",
    "    test['delinquency_12'] = test['delinquency_12'].fillna(-1)\n",
    "\n",
    "    joined_df = test.merge(everdf, how='left', on=['loan_id'], type='hash')\n",
    "    del (everdf)\n",
    "    del (test)\n",
    "\n",
    "    joined_df['ever_30'] = joined_df['ever_30'].fillna(-1)\n",
    "    joined_df['ever_90'] = joined_df['ever_90'].fillna(-1)\n",
    "    joined_df['ever_180'] = joined_df['ever_180'].fillna(-1)\n",
    "    joined_df['delinquency_30'] = joined_df['delinquency_30'].fillna(-1)\n",
    "    joined_df['delinquency_90'] = joined_df['delinquency_90'].fillna(-1)\n",
    "    joined_df['delinquency_180'] = joined_df['delinquency_180'].fillna(-1)\n",
    "\n",
    "    joined_df['timestamp_year'] = joined_df['timestamp_year'].astype('int32')\n",
    "    joined_df['timestamp_month'] = joined_df['timestamp_month'].astype('int32')\n",
    "\n",
    "    return joined_df\n",
    "\n",
    "\n",
    "# joined_df has loan_id, mrp_timestamp, timestamp_month, timestamp_year, delinquency_12, upb_12, ever_30, ever_90, ever_180, delinquency_30, delinquency_90, delinquency_180\n",
    "def create_12_mon_features(joined_df, **kwargs):\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "    all_results = []\n",
    "    testdfs = []\n",
    "    tables = {\"joined_df\": joined_df}\n",
    "    n_months = 12\n",
    "    for y in range(1, n_months + 1):\n",
    "        josh_mody_n_str = \"floor((timestamp_year * 12 + timestamp_month - 24000.0 - \" + str(y) + \")/12.0)\"\n",
    "        query = \"SELECT loan_id, \" + josh_mody_n_str + \" as josh_mody_n, max(delinquency_12) > 3 as max_d12_gt3, min(upb_12) = 0 as min_upb_12_eq0, min(upb_12) as upb_12  FROM main.joined_df as joined_df GROUP BY loan_id, \" + josh_mody_n_str\n",
    "        temp = pyblazing.run_query(query, tables)\n",
    "    \n",
    "        tables2 = {\"temp\": temp.columns}\n",
    "        query = \"SELECT loan_id, max_d12_gt3 + min_upb_12_eq0 as delinquency_12, upb_12, floor(((josh_mody_n * 12) + \" + str(\n",
    "            24000 + (y - 1)) + \")/12) as timestamp_year, josh_mody_n * 0 + \" + str(\n",
    "            y) + \" as timestamp_month from main.temp\"\n",
    "        results = pyblazing.run_query(query, tables2)\n",
    "        all_results.append(results)  # this is to keep them in scope\n",
    "        tmpdf = results.columns\n",
    "        testdfs.append(tmpdf)\n",
    "\n",
    "    result = cudf.concat(testdfs)\n",
    "    Chronometer.show(chronometer, 'Create 12 month features')\n",
    "    return result\n",
    "\n",
    "    # joined_df has loan_id, mrp_timestamp, timestamp_month, timestamp_year, delinquency_12, upb_12, ever_30, ever_90, ever_180, delinquency_30, delinquency_90, delinquency_180\n",
    "def create_12_mon_features_union(joined_df, **kwargs):\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "    all_results = []\n",
    "    all_temps = []\n",
    "    tables = {\"joined_df\": joined_df}\n",
    "    n_months = 12\n",
    "    for y in range(1, n_months + 1):\n",
    "        josh_mody_n_str = \"floor((timestamp_year * 12 + timestamp_month - 24000.0 - \" + str(y) + \")/12.0)\"\n",
    "        query = \"SELECT loan_id, \" + josh_mody_n_str + \" as josh_mody_n, max(delinquency_12) > 3 as max_d12_gt3, min(upb_12) = 0 as min_upb_12_eq0, min(upb_12) as upb_12  FROM main.joined_df as joined_df GROUP BY loan_id, \" + josh_mody_n_str\n",
    "        temp = pyblazing.run_query(query, tables)\n",
    "        all_temps.append(temp)\n",
    "  \n",
    "    y = 1\n",
    "    tables2 = {\"temp1\": all_temps[0].columns}\n",
    "    union_query = \"(SELECT loan_id, max_d12_gt3 + min_upb_12_eq0 as delinquency_12, upb_12, floor(((josh_mody_n * 12) + \" + str(\n",
    "            24000 + (y - 1)) + \")/12) as timestamp_year, josh_mody_n * 0 + \" + str(\n",
    "            y) + \" as timestamp_month from main.temp\" + str(y) + \")\"\n",
    "    for y in range(2, n_months + 1):\n",
    "        tables2[\"temp\" + str(y)] = all_temps[y-1].columns\n",
    "        query = \" UNION ALL (SELECT loan_id, max_d12_gt3 + min_upb_12_eq0 as delinquency_12, upb_12, floor(((josh_mody_n * 12) + \" + str(\n",
    "            24000 + (y - 1)) + \")/12) as timestamp_year, josh_mody_n * 0 + \" + str(\n",
    "            y) + \" as timestamp_month from main.temp\" + str(y) + \")\"\n",
    "        union_query = union_query + query\n",
    "\n",
    "    results = pyblazing.run_query(union_query, tables2)\n",
    "    Chronometer.show(chronometer, 'Create 12 month features union')\n",
    "\n",
    "    return results\n",
    "\n",
    "        # joined_df has loan_id, mrp_timestamp, timestamp_month, timestamp_year, delinquency_12, upb_12, ever_30, ever_90, ever_180, delinquency_30, delinquency_90, delinquency_180\n",
    "def create_12_mon_features_union_alt(joined_df, **kwargs):\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "    tables = {\"joined_df\": joined_df}\n",
    "    josh_mody_n_str = \"timestamp_year * 12 + timestamp_month - 24000.0\"\n",
    "    query = \"SELECT loan_id, \" + josh_mody_n_str + \" as josh_mody_n, max(delinquency_12) as max_d12, min(upb_12) as min_upb_12  FROM main.joined_df as joined_df GROUP BY loan_id, \" + josh_mody_n_str\n",
    "    mastertemp = pyblazing.run_query(query, tables)\n",
    "    \n",
    "    all_temps = []\n",
    "    all_tokens = []\n",
    "    tables = {\"joined_df\": mastertemp.columns}\n",
    "    n_months = 12\n",
    "    \n",
    "    for y in range(1, n_months + 1):\n",
    "        josh_mody_n_str = \"floor((josh_mody_n - \" + str(y) + \")/12.0)\"\n",
    "        query = \"SELECT loan_id, \" + josh_mody_n_str + \" as josh_mody_n, max(max_d12) > 3 as max_d12_gt3, min(min_upb_12) = 0 as min_upb_12_eq0, min(min_upb_12) as upb_12  FROM main.joined_df as joined_df GROUP BY loan_id, \" + josh_mody_n_str\n",
    "        \n",
    "        metaToken = pyblazing.run_query_get_token(query, tables)\n",
    "        all_tokens.append(metaToken)\n",
    "        \n",
    "    for metaToken in all_tokens:\n",
    "        temp = pyblazing.run_query_get_results(metaToken)\n",
    "        all_temps.append(temp)\n",
    "    \n",
    "    y = 1\n",
    "    tables2 = {\"temp1\": all_temps[0].columns}\n",
    "    union_query = \"(SELECT loan_id, max_d12_gt3 + min_upb_12_eq0 as delinquency_12, upb_12, floor(((josh_mody_n * 12) + \" + str(\n",
    "            24000 + (y - 1)) + \")/12) as timestamp_year, josh_mody_n * 0 + \" + str(\n",
    "            y) + \" as timestamp_month from main.temp\" + str(y) + \")\"\n",
    "    for y in range(2, n_months + 1):\n",
    "        tables2[\"temp\" + str(y)] = all_temps[y-1].columns\n",
    "        query = \" UNION ALL (SELECT loan_id, max_d12_gt3 + min_upb_12_eq0 as delinquency_12, upb_12, floor(((josh_mody_n * 12) + \" + str(\n",
    "            24000 + (y - 1)) + \")/12) as timestamp_year, josh_mody_n * 0 + \" + str(\n",
    "            y) + \" as timestamp_month from main.temp\" + str(y) + \")\"\n",
    "        union_query = union_query + query\n",
    "\n",
    "    results = pyblazing.run_query(union_query, tables2)\n",
    "    Chronometer.show(chronometer, 'Create 12 month features once')\n",
    "    return results\n",
    "\n",
    "\n",
    "def create_12_mon_features_old_rapids(joined_df, **kwargs):\n",
    "    testdfs = []\n",
    "    n_months = 12\n",
    "    for y in range(1, n_months + 1):\n",
    "        tmpdf = joined_df[['loan_id', 'timestamp_year', 'timestamp_month', 'delinquency_12', 'upb_12']]\n",
    "        print(tmpdf.dtypes)\n",
    "        tmpdf['josh_months'] = tmpdf['timestamp_year'] * 12 + tmpdf['timestamp_month']\n",
    "        tmpdf['josh_mody_n'] = ((tmpdf['josh_months'].astype('float64') - 24000 - y) / 12).floor()\n",
    "        tmpdf = tmpdf.groupby(['loan_id', 'josh_mody_n'], method='hash').agg({'delinquency_12': 'max', 'upb_12': 'min'})\n",
    "        tmpdf['delinquency_12'] = (tmpdf['max_delinquency_12'] > 3).astype('int32')\n",
    "        tmpdf['delinquency_12'] += (tmpdf['min_upb_12'] == 0).astype('int32')\n",
    "        tmpdf.drop_column('max_delinquency_12')\n",
    "        tmpdf['upb_12'] = tmpdf['min_upb_12']\n",
    "        tmpdf.drop_column('min_upb_12')\n",
    "        tmpdf['timestamp_year'] = (((tmpdf['josh_mody_n'] * n_months) + 24000 + (y - 1)) / 12).floor().astype('int16')\n",
    "        tmpdf['timestamp_month'] = np.int8(y)\n",
    "        tmpdf.drop_column('josh_mody_n')\n",
    "        testdfs.append(tmpdf)\n",
    "        del (tmpdf)\n",
    "    del (joined_df)\n",
    "\n",
    "    return cudf.concat(testdfs)\n",
    "\n",
    "\n",
    "# joined_df has loan_id, mrp_timestamp, timestamp_month, timestamp_year, delinquency_12, upb_12, ever_30, ever_90, ever_180, delinquency_30, delinquency_90, delinquency_180\n",
    "# testdf has loan_id delinquency_12 upb_12 timestamp_year timestamp_month\n",
    "def combine_joined_12_mon(joined_df, testdf, **kwargs):\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "    tables = {\"joined_df\": joined_df, \"testdf\": testdf}\n",
    "    query = \"\"\"SELECT j.loan_id, j.mrp_timestamp, j.timestamp_month, j.timestamp_year, \n",
    "                j.ever_30, j.ever_90, j.ever_180, j.delinquency_30, j.delinquency_90, j.delinquency_180,\n",
    "                t.delinquency_12, t.upb_12 \n",
    "                FROM main.joined_df as j LEFT OUTER JOIN main.testdf as t \n",
    "                ON j.loan_id = t.loan_id and j.timestamp_year = t.timestamp_year and j.timestamp_month = t.timestamp_month\"\"\"\n",
    "    results = pyblazing.run_query(query, tables)\n",
    "    Chronometer.show(chronometer, 'Combine joind 12 month')\n",
    "    return results\n",
    "\n",
    "\n",
    "def combine_joined_12_mon_old_rapids(joined_df, testdf, **kwargs):\n",
    "    joined_df.drop_column('delinquency_12')\n",
    "    joined_df.drop_column('upb_12')\n",
    "    joined_df['timestamp_year'] = joined_df['timestamp_year'].astype('int16')\n",
    "    joined_df['timestamp_month'] = joined_df['timestamp_month'].astype('int8')\n",
    "    return joined_df.merge(testdf, how='left', on=['loan_id', 'timestamp_year', 'timestamp_month'], type='hash')\n",
    "\n",
    "\n",
    "# joined_df has loan_id, mrp_timestamp, timestamp_month, timestamp_year, ever_30, ever_90, ever_180, delinquency_30, delinquency_90, delinquency_180, delinquency_12, upb_12\n",
    "def final_performance_delinquency(gdf, joined_df, **kwargs):\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "    tables = {\"gdf\": gdf, \"joined_df\": joined_df}\n",
    "    query = \"\"\"SELECT g.loan_id, current_actual_upb, current_loan_delinquency_status, delinquency_12, interest_rate, loan_age, mod_flag, msa, non_interest_bearing_upb \n",
    "        FROM main.gdf as g LEFT OUTER JOIN main.joined_df as j\n",
    "        ON g.loan_id = j.loan_id and EXTRACT(YEAR FROM g.monthly_reporting_period) = j.timestamp_year and EXTRACT(MONTH FROM g.monthly_reporting_period) = j.timestamp_month \"\"\"\n",
    "    results = pyblazing.run_query(query, tables)\n",
    "    Chronometer.show(chronometer, 'Final performance delinquency')\n",
    "    return results\n",
    "\n",
    "\n",
    "def final_performance_delinquency_old_rapids(gdf, joined_df, **kwargs):\n",
    "    merged = null_workaround(gdf)\n",
    "    joined_df = null_workaround(joined_df)\n",
    "    merged['timestamp_month'] = merged['monthly_reporting_period'].dt.month\n",
    "    merged['timestamp_month'] = merged['timestamp_month'].astype('int8')\n",
    "    merged['timestamp_year'] = merged['monthly_reporting_period'].dt.year\n",
    "    merged['timestamp_year'] = merged['timestamp_year'].astype('int16')\n",
    "    merged = merged.merge(joined_df, how='left', on=['loan_id', 'timestamp_year', 'timestamp_month'], type='hash')\n",
    "    merged.drop_column('timestamp_year')\n",
    "    merged.drop_column('timestamp_month')\n",
    "    return merged\n",
    "\n",
    "\n",
    "# from perf we need: current_actual_upb, current_loan_delinquency_status, delinquency_12, interest_rate, loan_age, mod_flag, msa, non_interest_bearing_upb\n",
    "\n",
    "# from acq we need: borrower_credit_score, dti, first_home_buyer, loan_purpose, mortgage_insurance_percent, num_borrowers, num_units, occupancy_status, \n",
    "# orig_channel, orig_cltv, orig_date, orig_interest_rate, orig_loan_term, orig_ltv, orig_upb, product_type, property_state, property_type, relocation_mortgage_indicator, seller_name, zip\n",
    "def join_perf_acq_gdfs(perf, acq, **kwargs):\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "    tables = {\"perf\": perf, \"acq\": acq}\n",
    "    query = \"\"\"SELECT p.loan_id, current_actual_upb, current_loan_delinquency_status, delinquency_12, interest_rate, loan_age, mod_flag, msa, non_interest_bearing_upb,\n",
    "     borrower_credit_score, dti, first_home_buyer, loan_purpose, mortgage_insurance_percent, num_borrowers, num_units, occupancy_status, \n",
    "     orig_channel, orig_cltv, orig_date, orig_interest_rate, orig_loan_term, orig_ltv, orig_upb, product_type, property_state, property_type, \n",
    "     relocation_mortgage_indicator, seller_name, zip FROM main.perf as p LEFT OUTER JOIN main.acq as a ON p.loan_id = a.loan_id\"\"\"\n",
    "    results = pyblazing.run_query(query, tables)\n",
    "    Chronometer.show(chronometer, 'Join performance acquitistion gdfs')\n",
    "    return results\n",
    "\n",
    "\n",
    "def join_perf_acq_gdfs_old_rapids(perf, acq, **kwargs):\n",
    "    perf = null_workaround(perf)\n",
    "    acq = null_workaround(acq)\n",
    "    return perf.merge(acq, how='left', on=['loan_id'], type='hash')\n",
    "\n",
    "\n",
    "def last_mile_cleaning(df, **kwargs):\n",
    "    # drop_list = [\n",
    "    #     'loan_id', 'orig_date', 'first_pay_date', 'seller_name',\n",
    "    #     'monthly_reporting_period', 'last_paid_installment_date', 'maturity_date', 'ever_30', 'ever_90', 'ever_180',\n",
    "    #     'delinquency_30', 'delinquency_90', 'delinquency_180', 'upb_12',\n",
    "    #     'zero_balance_effective_date','foreclosed_after', 'disposition_date','mrp_timestamp', 'loan_id0'\n",
    "    # ]\n",
    "    # for column in drop_list:\n",
    "    #     df.drop_column(column)\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "    for col, dtype in df.dtypes.iteritems():\n",
    "        if str(dtype) == 'category':\n",
    "            df[col] = df[col].cat.codes\n",
    "        df[col] = df[col].astype('float32')\n",
    "    df['delinquency_12'] = df['delinquency_12'] > 0\n",
    "    df['delinquency_12'] = df['delinquency_12'].fillna(False).astype('int32')\n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].fillna(-1)\n",
    "    Chronometer.show(chronometer, 'Last mile cleaning')\n",
    "    return df\n",
    "  \n",
    "\n",
    "use_hdfs = True\n",
    "\n",
    "# to download data for this notebook, visit https://rapidsai.github.io/demos/datasets/mortgage-data and update the following paths accordingly\n",
    "\n",
    "acq_data_path = \"\"\n",
    "perf_data_path = \"\"\n",
    "col_names_path = \"\"\n",
    "if use_hdfs:\n",
    "    acq_data_path = \"hdfs://35.203.181.221/mortgage_2000-2001/acq\"\n",
    "    perf_data_path = \"hdfs://35.203.181.221/mortgage_2000-2001/perf\"\n",
    "    col_names_path = \"hdfs://35.203.181.221/mortgage_2000-2001/names.csv\"\n",
    "else:\n",
    "    acq_data_path = \"/home/william/repos/blazingsql-mortgage-pipeline/data/acq\"\n",
    "    perf_data_path = \"/home/william/repos/blazingsql-mortgage-pipeline/data/perf\"\n",
    "    col_names_path = \"/home/william/repos/blazingsql-mortgage-pipeline/data/names.csv\"\n",
    "\n",
    "start_year = 2000\n",
    "end_year = 2000  # end_year is inclusive\n",
    "start_quarter = 1\n",
    "end_quarter = 1\n",
    "part_count = 1  # the number of data files to train against\n",
    "\n",
    "import time\n",
    "\n",
    "# initialize_rmm_pool()\n",
    "\n",
    "dxgb_gpu_params = {\n",
    "    'nround':            100,\n",
    "    'max_depth':         8,\n",
    "    'max_leaves':        2**8,\n",
    "    'alpha':             0.9,\n",
    "    'eta':               0.1,\n",
    "    'gamma':             0.1,\n",
    "    'learning_rate':     0.1,\n",
    "    'subsample':         1,\n",
    "    'reg_lambda':        1,\n",
    "    'scale_pos_weight':  2,\n",
    "    'min_child_weight':  30,\n",
    "    'tree_method':       'gpu_hist',\n",
    "    'n_gpus':            1,\n",
    "    'distributed_dask':  True,\n",
    "    'loss':              'ls',\n",
    "    'objective':         'reg:linear',\n",
    "    'max_features':      'auto',\n",
    "    'criterion':         'friedman_mse',\n",
    "    'grow_policy':       'lossguide',\n",
    "    # 'nthread', ncores[worker],  # WSM may want to set this\n",
    "    'verbose':           True\n",
    "}\n",
    "\n",
    "\n",
    "def range1(start, end):\n",
    "    return range(start, end+1)\n",
    "\n",
    "def use_file_type_suffix(year, quarter):\n",
    "    if year==2001 and quarter>=2:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def getChunks(year, quarter):\n",
    "    if use_file_type_suffix(year, quarter):\n",
    "        return range(0, 1+1)\n",
    "    return range(0, 0+1)\n",
    "\n",
    "final_cpu_df_label = None\n",
    "final_cpu_df_data = None\n",
    "\n",
    "all_load_times = []\n",
    "all_etl_times = []\n",
    "all_xgb_convert_times = []\n",
    "\n",
    "for year in range1(start_year, end_year):\n",
    "    for quarter in range1(start_quarter, end_quarter):\n",
    "        for chunk in getChunks(year, quarter):\n",
    "            chunk_sufix = \"_{}\".format(chunk) if use_file_type_suffix(year, quarter) else \"\"\n",
    "            perf_file = perf_data_path + \"/Performance_\" + str(year) + \"Q\" + str(quarter) + \".txt\" + chunk_sufix\n",
    "        \n",
    "            [gpu_df, load_time, etl_time] = run_gpu_workflow(quarter=quarter, year=year, perf_file=perf_file)\n",
    "            all_load_times.append(load_time)\n",
    "            all_etl_times.append(etl_time)\n",
    "\n",
    "            xgb_convert_start_time = time.time()\n",
    "\n",
    "            gpu_df = (gpu_df[['delinquency_12']], gpu_df[list(gpu_df.columns.difference(['delinquency_12']))])\n",
    "\n",
    "            cpu_df_label = gpu_df[0].to_pandas()\n",
    "            cpu_df_data = gpu_df[1].to_pandas()\n",
    "\n",
    "            del (gpu_df)\n",
    "\n",
    "            if year == start_year:\n",
    "                final_cpu_df_label = cpu_df_label\n",
    "                final_cpu_df_data = cpu_df_data\n",
    "            else:\n",
    "                final_cpu_df_label = pd.concat([final_cpu_df_label, cpu_df_label])\n",
    "                final_cpu_df_data = pd.concat([final_cpu_df_data, cpu_df_data])\n",
    "\n",
    "            xgb_convert_end_time = time.time()\n",
    "\n",
    "            all_xgb_convert_times.append(xgb_convert_end_time - xgb_convert_start_time)\n",
    "\n",
    "data_train, data_test, label_train, label_test = train_test_split(final_cpu_df_data, final_cpu_df_label, test_size=0.20, random_state=42)\n",
    "\n",
    "xgdf_train = xgb.DMatrix(data_train, label_train)\n",
    "xgdf_test = xgb.DMatrix(data_test, label_test)\n",
    "\n",
    "# labels = None\n",
    "# bst = dxgb_gpu.train(client, dxgb_gpu_params, gpu_dfs, labels, num_boost_round=dxgb_gpu_params['nround'])\n",
    "\n",
    "# BEGIN predict 1\n",
    "chronometerTrain1 = Chronometer.makeStarted()\n",
    "startTime = time.time()\n",
    "bst = xgb.train(dxgb_gpu_params, xgdf_train, num_boost_round=dxgb_gpu_params['nround'])\n",
    "Chronometer.show(chronometerTrain1, 'Train 1')\n",
    "\n",
    "chronometerPredict1 = Chronometer.makeStarted()\n",
    "preds = bst.predict(xgdf_test)\n",
    "Chronometer.show(chronometerPredict1, 'Predict 1')\n",
    "\n",
    "labels = xgdf_test.get_label()\n",
    "print('prediction error=%f' % (sum(1 for i in range(len(preds)) if int(preds[i] > 0.5) != labels[i]) / float(len(preds))))\n",
    "\n",
    "endTime = time.time()\n",
    "trainPredict_time = (endTime - startTime)\n",
    "\n",
    "print(\"TIMES SUMMARY\")\n",
    "print('LOAD Time: %fs' % sum(all_load_times))\n",
    "print('ETL Time: %fs' % sum(all_etl_times))\n",
    "print('CONVERT Time: %fs' % sum(all_xgb_convert_times))\n",
    "print('TRAIN/PREDICT Time: %fs' % trainPredict_time)\n",
    "\n",
    "\n",
    "# END predict 1\n",
    "\n",
    "# BEGIN predict 2\n",
    "# def feval(preds, dtrain):\n",
    "#     labels = dtrain.get_label()\n",
    "#     error = float(sum(labels != (preds > 0.5))) / len(labels)\n",
    "#     return 'bsql-error', error\n",
    "# startTime = time.time()\n",
    "# bst = xgb.train(dxgb_gpu_params, xgdf_train, dxgb_gpu_params['nround'], [(xgdf_test, 'eval'), (xgdf_train, 'train')], feval=feval)\n",
    "# endTime = time.time()\n",
    "# print(bst)\n",
    "# chronometerPredict2 = Chronometer.makeStarted()\n",
    "# preds = bst.predict(xgdf_test)\n",
    "# Chronometer.show(chronometerPredict2, 'Predict 2')\n",
    "# print(preds)\n",
    "# # END predict 2\n",
    "Chronometer.show_resume()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
