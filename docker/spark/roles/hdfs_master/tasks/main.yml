- name: Add the user 'hadoop'
  user:
    name: hadoop
    groups: adm
    shell: /bin/bash
    create_home: true
  tags: ADD

- name: Assign password hadoop user
  shell: echo "hadoop:hadoop" | sudo chpasswd
  become: true
  tags: passwd

- name: Update repositories
  apt:
    update_cache: yes
  become: true

- name: Install dependencies
  apt:
    name: "{{ item }}"
    state: present
  with_items:
    - openjdk-8-jdk-headless
    - sshpass
  become: true

- name: Installing java
  apt_repository: repo='ppa:openjdk-r/ppa'

- name: Set JAVA_HOME
  lineinfile: dest=/etc/environment state=present regexp='^JAVA_HOME' line='JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64'

- name: download hadoop
  get_url:
    url: http://www-us.apache.org/dist/hadoop/common/hadoop-2.8.4/hadoop-2.8.4.tar.gz
    dest: /home/hadoop
    mode: 0777 
    owner: hadoop
    group: hadoop
  become: true
  
- name: Extract hadoop installer
  unarchive:
    src: /home/hadoop/hadoop-2.8.4.tar.gz
    dest: /home/hadoop
    remote_src: true 
    owner: hadoop
    group: hadoop
    mode: 0777
  become: true

- name: copying files to master
  copy:
    src: "{{ item }}"
    dest: /home/hadoop/hadoop-2.8.4/etc/hadoop/
    owner: hadoop
    group: hadoop
    mode: 0777
  become: true
  with_items:
    - core-site.xml
    - hdfs-site.xml

- name: Creates folders in masters
  file:
    path: /home/hadoop/{{ item }}
    state: directory
    mode: 0764
    owner: hadoop
    group: hadoop
    recurse: yes
  with_items:
    - ["tmp","name"]
  become: true

- name: Clean JAVA_HOME 
  shell: sed -i '/export JAVA_HOME=${JAVA_HOME}/d' ./hadoop-env.sh
  args:
    chdir:  /home/hadoop/hadoop-2.8.4/etc/hadoop
  become: true
  become_user: hadoop

- name: Add JAVA_HOME
  lineinfile:
      path: /home/hadoop/hadoop-2.8.4/etc/hadoop/hadoop-env.sh
      insertafter: '# The java implementation to use.'
      line: 'export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64'
  become_user: hadoop

- name: Move Home Hadoop
  command: mv /home/hadoop/hadoop-2.8.4 /usr/local/hadoop
  become: true
    

- name: Modified /etc/hosts
  lineinfile: 
    path: /etc/hosts
    insertafter: '127.0.0.1 localhost'
    state: present
    line: '127.0.0.1 hadoop-master'

- name: Format HDFS
  shell: yes | ./hdfs namenode -format
  args:
    chdir: /usr/local/hadoop/bin
  become: true

- name: Delete path /usr/local/hadoop into /etc/enviroment
  command: sed -e 's|/usr/local/hadoop:||g' -i /etc/environment 
  become: true

- name: Append path /usr/local/hadoop into /etc/enviroment
  command: sed -e 's|PATH="\(.*\)"|PATH="/usr/local/hadoop:\1"|g' -i /etc/environment
  become: true



#STEP-00
#

#STEP-01
#Master
#ssh-keygen
#ssh-copy-id hadoop@hadoop-slave1

#STEP-02
#Master y slaves
#sudo nano /etc/hosts
#127.0.0.1	masterhdfs
#127.0.0.1       localhost
#192.168.100.101	masterhdfs
#192.168.100.101 hadoop-master
#192.168.100.102 hadoop-slave1

#STEP-03: Añadir ips de workers into slaves file
#Master y slaves
#sudo nano /usr/local/hadoop/etc/hadoop/slaves
#hadoop-slave1

#STEP-04: In cloud this step is necesary
#port: open 54310 port ingress

# Start namenode manual (3)
# cd /usr/local/hadoop/sbin
#./hadoop-daemon.sh --script hdfs start namenode

#jps
#Result
#jps
#27082 SecondaryNameNode
#27242 ResourceManager
#26859 NameNode
#27503 Jps


#TEST
# cd /usr/local/hadoop/bin
# ./hdfs dfs -ls /.
#./hdfs dfs -mkdir /edith
#./hdfs dfs -ls /.
#./hdfs dfs -copyFromLocal hola.txt /edith
#./hdfs dfs -mkdir /edith
#./hdfs dfs -ls /edith
#./hdfs dfs -cat /edith/hola.txt



OJOO

cambiar permisos a log
ssh hadoop@hadoop-slave1


#ssh-keygen -t rsa
#ssh-copy-id hadoop@localhost
**no debe perir contrasña
ssh-copy-id hadoop@hadoop-master
ssh-copy-id hadoop@hadoop-masalve

start-all.sh

# master inciia a slave
copiarllaves
registrar slaves ( en master)
registrar 
etc/hosts ( en master)

#nano hdfs-site.xml ( cambiar a 1 replication)

borrar hadoo-master de /etc/hosts de slaves

stop all y satar-all


# sudo su hadoop, enter with user hadoop


