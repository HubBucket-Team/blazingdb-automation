{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Register a POSIX File System ***\n",
      "1\n",
      "TIMES SUMMARY Total Elapsed on all machines\n",
      "LOAD Time: 1.539484s\n",
      "ETL Time: 4.219862s\n",
      "Wall Time 11.383793s\n",
      "Wall LOAD Time: 3.042909s\n",
      "Wall ETL Time: 8.340883s\n",
      "1\n",
      "\u001b[32mResume\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import cudf\n",
    "from cudf.dataframe import DataFrame\n",
    "from collections import OrderedDict\n",
    "import gc\n",
    "from glob import glob\n",
    "import os\n",
    "import pyblazing\n",
    "import pandas as pd\n",
    "import time\n",
    "from dask.distributed import Client\n",
    "from chronometer import Chronometer\n",
    "\n",
    "from pyblazing import FileSystemType, SchemaFrom, DriverType\n",
    "\n",
    "def register_hdfs():\n",
    "    print('*** Register a HDFS File System ***')\n",
    "    fs_status = pyblazing.register_file_system(\n",
    "        authority=\"myLocalHdfs\",\n",
    "        type=FileSystemType.HDFS,\n",
    "        root=\"/\",\n",
    "        params={\n",
    "            \"host\": \"127.0.0.1\",\n",
    "            \"port\": 54310,\n",
    "            \"user\": \"hadoop\",\n",
    "            \"driverType\": DriverType.LIBHDFS3,\n",
    "            \"kerberosTicket\": \"\"\n",
    "        }\n",
    "    )\n",
    "    print(fs_status)\n",
    "\n",
    "\n",
    "def deregister_hdfs():\n",
    "    fs_status = pyblazing.deregister_file_system(authority=\"myLocalHdfs\")\n",
    "    print(fs_status)\n",
    "\n",
    "def register_posix():\n",
    "\n",
    "    print('*** Register a POSIX File System ***')\n",
    "    fs_status = pyblazing.register_file_system(\n",
    "        authority=\"mortgage\",\n",
    "        type=FileSystemType.POSIX,\n",
    "        root=\"/\"\n",
    "    )\n",
    "    print(fs_status)\n",
    "\n",
    "def deregister_posix():\n",
    "    fs_status = pyblazing.deregister_file_system(authority=\"mortgage\")\n",
    "    print(fs_status)\n",
    "\n",
    "from libgdf_cffi import ffi, libgdf\n",
    "\n",
    "def get_dtype_values(dtypes):\n",
    "    values = []\n",
    "    def gdf_type(type_name):\n",
    "        dicc = {\n",
    "            'str': libgdf.GDF_STRING,\n",
    "            'date': libgdf.GDF_DATE64,\n",
    "            'date64': libgdf.GDF_DATE64,\n",
    "            'date32': libgdf.GDF_DATE32,\n",
    "            'timestamp': libgdf.GDF_TIMESTAMP,\n",
    "            'category': libgdf.GDF_CATEGORY,\n",
    "            'float': libgdf.GDF_FLOAT32,\n",
    "            'double': libgdf.GDF_FLOAT64,\n",
    "            'float32': libgdf.GDF_FLOAT32,\n",
    "            'float64': libgdf.GDF_FLOAT64,\n",
    "            'short': libgdf.GDF_INT16,\n",
    "            'long': libgdf.GDF_INT64,\n",
    "            'int': libgdf.GDF_INT32,\n",
    "            'int32': libgdf.GDF_INT32,\n",
    "            'int64': libgdf.GDF_INT64,\n",
    "        }\n",
    "        if dicc.get(type_name):\n",
    "            return dicc[type_name]\n",
    "        return libgdf.GDF_INT64\n",
    "\n",
    "    for key in dtypes:\n",
    "        values.append( gdf_type(dtypes[key]))\n",
    "\n",
    "    print('>>>> dtyps for', dtypes.values())\n",
    "    print(values)\n",
    "    return values\n",
    "\n",
    "def get_type_schema(path):\n",
    "    format = path.split('.')[-1]\n",
    "\n",
    "    if format == 'parquet':\n",
    "        return SchemaFrom.ParquetFile\n",
    "    elif format == 'csv' or format == 'psv' or format.startswith(\"txt\"):\n",
    "        return SchemaFrom.CsvFile\n",
    "\n",
    "def open_perf_table(table_ref):\n",
    "    for key in table_ref.keys():\n",
    "        sql = 'select * from main.%(table_name)s' % {\"table_name\": key.table_name}\n",
    "        return pyblazing.run_query(sql, table_ref)\n",
    "\n",
    "\n",
    "def run_gpu_workflow(params):\n",
    "    \n",
    "    import os\n",
    "\n",
    "    quarter=params['quarter']\n",
    "    year=params['year']\n",
    "    perf_file=params['perf_file']\n",
    "    \n",
    "    print(\"params:\", str(quarter), str(year), str(perf_file))\n",
    "    os.system(\"/blazingdb/notebooks/start_conda_gpu.sh \" + str(quarter) + \" \" + str(year) + \" \" +  str(perf_file))\n",
    "    return [1, 1]\n",
    "\n",
    "def gpu_load_performance_csv(performance_path, **kwargs):\n",
    "    \"\"\" Loads performance data\n",
    "    Returns\n",
    "    -------\n",
    "    GPU DataFrame\n",
    "    \"\"\"\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "\n",
    "    cols = [\n",
    "        \"loan_id\", \"monthly_reporting_period\", \"servicer\", \"interest_rate\", \"current_actual_upb\",\n",
    "        \"loan_age\", \"remaining_months_to_legal_maturity\", \"adj_remaining_months_to_maturity\",\n",
    "        \"maturity_date\", \"msa\", \"current_loan_delinquency_status\", \"mod_flag\", \"zero_balance_code\",\n",
    "        \"zero_balance_effective_date\", \"last_paid_installment_date\", \"foreclosed_after\",\n",
    "        \"disposition_date\", \"foreclosure_costs\", \"prop_preservation_and_repair_costs\",\n",
    "        \"asset_recovery_costs\", \"misc_holding_expenses\", \"holding_taxes\", \"net_sale_proceeds\",\n",
    "        \"credit_enhancement_proceeds\", \"repurchase_make_whole_proceeds\", \"other_foreclosure_proceeds\",\n",
    "        \"non_interest_bearing_upb\", \"principal_forgiveness_upb\", \"repurchase_make_whole_proceeds_flag\",\n",
    "        \"foreclosure_principal_write_off_amount\", \"servicing_activity_indicator\"\n",
    "    ]\n",
    "\n",
    "    dtypes = OrderedDict([\n",
    "        (\"loan_id\", \"int64\"),\n",
    "        (\"monthly_reporting_period\", \"date\"),\n",
    "        (\"servicer\", \"category\"),\n",
    "        (\"interest_rate\", \"float64\"),\n",
    "        (\"current_actual_upb\", \"float64\"),\n",
    "        (\"loan_age\", \"float64\"),\n",
    "        (\"remaining_months_to_legal_maturity\", \"float64\"),\n",
    "        (\"adj_remaining_months_to_maturity\", \"float64\"),\n",
    "        (\"maturity_date\", \"date\"),\n",
    "        (\"msa\", \"float64\"),\n",
    "        (\"current_loan_delinquency_status\", \"int32\"),\n",
    "        (\"mod_flag\", \"category\"),\n",
    "        (\"zero_balance_code\", \"category\"),\n",
    "        (\"zero_balance_effective_date\", \"date\"),\n",
    "        (\"last_paid_installment_date\", \"date\"),\n",
    "        (\"foreclosed_after\", \"date\"),\n",
    "        (\"disposition_date\", \"date\"),\n",
    "        (\"foreclosure_costs\", \"float64\"),\n",
    "        (\"prop_preservation_and_repair_costs\", \"float64\"),\n",
    "        (\"asset_recovery_costs\", \"float64\"),\n",
    "        (\"misc_holding_expenses\", \"float64\"),\n",
    "        (\"holding_taxes\", \"float64\"),\n",
    "        (\"net_sale_proceeds\", \"float64\"),\n",
    "        (\"credit_enhancement_proceeds\", \"float64\"),\n",
    "        (\"repurchase_make_whole_proceeds\", \"float64\"),\n",
    "        (\"other_foreclosure_proceeds\", \"float64\"),\n",
    "        (\"non_interest_bearing_upb\", \"float64\"),\n",
    "        (\"principal_forgiveness_upb\", \"float64\"),\n",
    "        (\"repurchase_make_whole_proceeds_flag\", \"category\"),\n",
    "        (\"foreclosure_principal_write_off_amount\", \"float64\"),\n",
    "        (\"servicing_activity_indicator\", \"category\")\n",
    "    ])\n",
    "    print(performance_path)\n",
    "    performance_table = pyblazing.create_table(table_name='perf', type=get_type_schema(performance_path), path=performance_path, delimiter='|', names=cols, dtypes=get_dtype_values(dtypes), skip_rows=1)\n",
    "    Chronometer.show(chronometer, 'Read Performance CSV')\n",
    "    return performance_table\n",
    "\n",
    "def gpu_load_acquisition_csv(acquisition_path, **kwargs):\n",
    "    \"\"\" Loads acquisition data\n",
    "    Returns\n",
    "    -------\n",
    "    GPU DataFrame\n",
    "    \"\"\"\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "\n",
    "    cols = [\n",
    "        'loan_id', 'orig_channel', 'seller_name', 'orig_interest_rate', 'orig_upb', 'orig_loan_term',\n",
    "        'orig_date', 'first_pay_date', 'orig_ltv', 'orig_cltv', 'num_borrowers', 'dti', 'borrower_credit_score',\n",
    "        'first_home_buyer', 'loan_purpose', 'property_type', 'num_units', 'occupancy_status', 'property_state',\n",
    "        'zip', 'mortgage_insurance_percent', 'product_type', 'coborrow_credit_score', 'mortgage_insurance_type',\n",
    "        'relocation_mortgage_indicator'\n",
    "    ]\n",
    "\n",
    "    dtypes = OrderedDict([\n",
    "        (\"loan_id\", \"int64\"),\n",
    "        (\"orig_channel\", \"category\"),\n",
    "        (\"seller_name\", \"category\"),\n",
    "        (\"orig_interest_rate\", \"float64\"),\n",
    "        (\"orig_upb\", \"int64\"),\n",
    "        (\"orig_loan_term\", \"int64\"),\n",
    "        (\"orig_date\", \"date\"),\n",
    "        (\"first_pay_date\", \"date\"),\n",
    "        (\"orig_ltv\", \"float64\"),\n",
    "        (\"orig_cltv\", \"float64\"),\n",
    "        (\"num_borrowers\", \"float64\"),\n",
    "        (\"dti\", \"float64\"),\n",
    "        (\"borrower_credit_score\", \"float64\"),\n",
    "        (\"first_home_buyer\", \"category\"),\n",
    "        (\"loan_purpose\", \"category\"),\n",
    "        (\"property_type\", \"category\"),\n",
    "        (\"num_units\", \"int64\"),\n",
    "        (\"occupancy_status\", \"category\"),\n",
    "        (\"property_state\", \"category\"),\n",
    "        (\"zip\", \"int64\"),\n",
    "        (\"mortgage_insurance_percent\", \"float64\"),\n",
    "        (\"product_type\", \"category\"),\n",
    "        (\"coborrow_credit_score\", \"float64\"),\n",
    "        (\"mortgage_insurance_type\", \"float64\"),\n",
    "        (\"relocation_mortgage_indicator\", \"category\")\n",
    "    ])\n",
    "\n",
    "    print(acquisition_path)\n",
    "\n",
    "    acquisition_table = pyblazing.create_table(table_name='acq', type=get_type_schema(acquisition_path), path=acquisition_path, delimiter='|', names=cols, dtypes=get_dtype_values(dtypes), skip_rows=1)\n",
    "    Chronometer.show(chronometer, 'Read Acquisition CSV')\n",
    "    return acquisition_table\n",
    "\n",
    "def gpu_load_names(**kwargs):\n",
    "    \"\"\" Loads names used for renaming the banks\n",
    "    Returns\n",
    "    -------\n",
    "    GPU DataFrame\n",
    "    \"\"\"\n",
    "#     chronometer = Chronometer.makeStarted()\n",
    "\n",
    "    cols = [\n",
    "        'seller_name', 'new_seller_name'\n",
    "    ]\n",
    "\n",
    "    dtypes = OrderedDict([\n",
    "        (\"seller_name\", \"category\"),\n",
    "        (\"new_seller_name\", \"category\"),\n",
    "    ])\n",
    "\n",
    "    names_table = pyblazing.create_table(table_name='names', type=get_type_schema(col_names_path), path=col_names_path, delimiter='|', names=cols, dtypes=get_dtype_values(dtypes), skip_rows=1)\n",
    "#     Chronometer.show(chronometer, 'Read Names CSV')\n",
    "    return names_table\n",
    "\n",
    "\n",
    "def merge_names(names_table, acq_table):\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "    tables = {names_table.name: names_table.columns,\n",
    "              acq_table.name:acq_table.columns}\n",
    "\n",
    "    query = \"\"\"SELECT loan_id, orig_channel, orig_interest_rate, orig_upb, orig_loan_term,\n",
    "        orig_date, first_pay_date, orig_ltv, orig_cltv, num_borrowers, dti, borrower_credit_score,\n",
    "        first_home_buyer, loan_purpose, property_type, num_units, occupancy_status, property_state,\n",
    "        zip, mortgage_insurance_percent, product_type, coborrow_credit_score, mortgage_insurance_type,\n",
    "        relocation_mortgage_indicator, new_seller_name as seller_name\n",
    "        FROM main.acq as a LEFT OUTER JOIN main.names as n ON  a.seller_name = n.seller_name\"\"\"\n",
    "    result = pyblazing.run_query(query, tables)\n",
    "    Chronometer.show(chronometer, 'Create Acquisition (Merge Names)')\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_ever_features(table, **kwargs):\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "    query = \"\"\"SELECT loan_id,\n",
    "        max(current_loan_delinquency_status) >= 1 as ever_30,\n",
    "        max(current_loan_delinquency_status) >= 3 as ever_90,\n",
    "        max(current_loan_delinquency_status) >= 6 as ever_180\n",
    "        FROM main.perf group by loan_id\"\"\"\n",
    "    result = pyblazing.run_query(query, {table.name: table.columns})\n",
    "    Chronometer.show(chronometer, 'Create Ever Features')\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_delinq_features(table, **kwargs):\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "    query = \"\"\"SELECT loan_id,\n",
    "        min(monthly_reporting_period) as delinquency_30\n",
    "        FROM main.perf where current_loan_delinquency_status >= 1 group by loan_id\"\"\"\n",
    "    result_delinq_30 = pyblazing.run_query(query, {table.name: table.columns})\n",
    "\n",
    "    query = \"\"\"SELECT loan_id,\n",
    "        min(monthly_reporting_period) as delinquency_90\n",
    "        FROM main.perf where current_loan_delinquency_status >= 3 group by loan_id\"\"\"\n",
    "    result_delinq_90 = pyblazing.run_query(query, {table.name: table.columns})\n",
    "\n",
    "    query = \"\"\"SELECT loan_id,\n",
    "        min(monthly_reporting_period) as delinquency_180\n",
    "        FROM main.perf where current_loan_delinquency_status >= 6 group by loan_id\"\"\"\n",
    "    result_delinq_180 = pyblazing.run_query(query, {table.name: table.columns})\n",
    "\n",
    "\n",
    "\n",
    "    new_tables = {\"delinq_30\": result_delinq_30.columns, \"delinq_90\": result_delinq_90.columns, \"delinq_180\": result_delinq_180.columns}\n",
    "    query = \"\"\"SELECT d30.loan_id, delinquency_30, delinquency_90,\n",
    "                delinquency_180 FROM main.delinq_30 as d30\n",
    "                LEFT OUTER JOIN main.delinq_90 as d90 ON d30.loan_id = d90.loan_id\n",
    "                LEFT OUTER JOIN main.delinq_180 as d180 ON d30.loan_id = d180.loan_id\"\"\"\n",
    "    result_merge = pyblazing.run_query(query, new_tables)\n",
    "    print(query)\n",
    "    print(result_merge)\n",
    "    print(result_merge.columns)\n",
    "    if result_merge.columns['delinquency_90'].has_null_mask:\n",
    "        result_merge.columns['delinquency_90'] = result_merge.columns['delinquency_90'].fillna(\n",
    "            np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "\n",
    "    if result_merge.columns['delinquency_180'].has_null_mask:\n",
    "        result_merge.columns['delinquency_180'] = result_merge.columns['delinquency_180'].fillna(\n",
    "            np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "\n",
    "    Chronometer.show(chronometer, 'Create deliquency features')\n",
    "    return result_merge\n",
    "\n",
    "\n",
    "def join_ever_delinq_features(everdf_tmp, delinq_merge, **kwargs):\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "    tables = {\"everdf\": everdf_tmp, \"delinq\": delinq_merge}\n",
    "    query = \"\"\"SELECT everdf.loan_id as loan_id, ever_30, ever_90, ever_180,\n",
    "                  delinquency_30,\n",
    "                  delinquency_90,\n",
    "                  delinquency_180 FROM main.everdf as everdf\n",
    "                  LEFT OUTER JOIN main.delinq as delinq ON everdf.loan_id = delinq.loan_id\"\"\"\n",
    "    result_merge = pyblazing.run_query(query, tables)\n",
    "    if result_merge.columns['delinquency_30'].has_null_mask:\n",
    "        result_merge.columns['delinquency_30'] = result_merge.columns['delinquency_30'].fillna(\n",
    "            np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    if result_merge.columns['delinquency_90'].has_null_mask:\n",
    "        result_merge.columns['delinquency_90'] = result_merge.columns['delinquency_90'].fillna(\n",
    "            np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    if result_merge.columns['delinquency_180'].has_null_mask:\n",
    "        result_merge.columns['delinquency_180'] = result_merge.columns['delinquency_180'].fillna(\n",
    "            np.dtype('datetime64[ms]').type('1970-01-01').astype('datetime64[ms]'))\n",
    "    Chronometer.show(chronometer, 'Create ever deliquency features')\n",
    "    return result_merge\n",
    "\n",
    "\n",
    "def create_joined_df(gdf, everdf, **kwargs):\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "    tables = {\"perf\": gdf, \"everdf\": everdf}\n",
    "\n",
    "    query = \"\"\"SELECT perf.loan_id as loan_id,\n",
    "                perf.monthly_reporting_period as mrp_timestamp,\n",
    "                EXTRACT(MONTH FROM perf.monthly_reporting_period) as timestamp_month,\n",
    "                EXTRACT(YEAR FROM perf.monthly_reporting_period) as timestamp_year,\n",
    "                perf.current_loan_delinquency_status as delinquency_12,\n",
    "                perf.current_actual_upb as upb_12,\n",
    "                everdf.ever_30 as ever_30,\n",
    "                everdf.ever_90 as ever_90,\n",
    "                everdf.ever_180 as ever_180,\n",
    "                everdf.delinquency_30 as delinquency_30,\n",
    "                everdf.delinquency_90 as delinquency_90,\n",
    "                everdf.delinquency_180 as delinquency_180\n",
    "                FROM main.perf as perf\n",
    "                LEFT OUTER JOIN main.everdf as everdf ON perf.loan_id = everdf.loan_id\"\"\"\n",
    "\n",
    "    results = pyblazing.run_query(query, tables)\n",
    "\n",
    "    if results.columns['upb_12'].has_null_mask:\n",
    "        results.columns['upb_12'] = results.columns['upb_12'].fillna(999999999)\n",
    "    if results.columns['delinquency_12'].has_null_mask:\n",
    "        results.columns['delinquency_12'] = results.columns['delinquency_12'].fillna(-1)\n",
    "    if results.columns['ever_30'].has_null_mask:\n",
    "        results.columns['ever_30'] = results.columns['ever_30'].astype('int8').fillna(-1)\n",
    "    if results.columns['ever_90'].has_null_mask:\n",
    "        results.columns['ever_90'] = results.columns['ever_90'].astype('int8').fillna(-1)\n",
    "    if results.columns['ever_180'].has_null_mask:\n",
    "        results.columns['ever_180'] = results.columns['ever_180'].astype('int8').fillna(-1)\n",
    "    if results.columns['delinquency_30'].has_null_mask:\n",
    "        results.columns['delinquency_30'] = results.columns['delinquency_30'].fillna(-1)\n",
    "    if results.columns['delinquency_90'].has_null_mask:\n",
    "        results.columns['delinquency_90'] = results.columns['delinquency_90'].fillna(-1)\n",
    "    if results.columns['delinquency_180'].has_null_mask:\n",
    "        results.columns['delinquency_180'] = results.columns['delinquency_180'].fillna(-1)\n",
    "\n",
    "    Chronometer.show(chronometer, 'Create Joined DF')\n",
    "    return results\n",
    "\n",
    "\n",
    "def create_12_mon_features_union(joined_df, **kwargs):\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "    tables = {\"joined_df\": joined_df}\n",
    "    josh_mody_n_str = \"timestamp_year * 12 + timestamp_month - 24000.0\"\n",
    "    query = \"SELECT loan_id, \" + josh_mody_n_str + \" as josh_mody_n, max(delinquency_12) as max_d12, min(upb_12) as min_upb_12  FROM main.joined_df as joined_df GROUP BY loan_id, \" + josh_mody_n_str\n",
    "    mastertemp = pyblazing.run_query(query, tables)\n",
    "\n",
    "    all_temps = []\n",
    "    all_tokens = []\n",
    "    tables = {\"joined_df\": mastertemp.columns}\n",
    "    n_months = 12\n",
    "\n",
    "    for y in range(1, n_months + 1):\n",
    "        josh_mody_n_str = \"floor((josh_mody_n - \" + str(y) + \")/12.0)\"\n",
    "        query = \"SELECT loan_id, \" + josh_mody_n_str + \" as josh_mody_n, max(max_d12) > 3 as max_d12_gt3, min(min_upb_12) = 0 as min_upb_12_eq0, min(min_upb_12) as upb_12  FROM main.joined_df as joined_df GROUP BY loan_id, \" + josh_mody_n_str\n",
    "\n",
    "        metaToken = pyblazing.run_query_get_token(query, tables)\n",
    "        all_tokens.append(metaToken)\n",
    "\n",
    "    for metaToken in all_tokens:\n",
    "        temp = pyblazing.run_query_get_results(metaToken)\n",
    "        all_temps.append(temp)\n",
    "\n",
    "    y = 1\n",
    "    tables2 = {\"temp1\": all_temps[0].columns}\n",
    "    union_query = \"(SELECT loan_id, max_d12_gt3 + min_upb_12_eq0 as delinquency_12, upb_12, floor(((josh_mody_n * 12) + \" + str(\n",
    "            24000 + (y - 1)) + \")/12) as timestamp_year, josh_mody_n * 0 + \" + str(\n",
    "            y) + \" as timestamp_month from main.temp\" + str(y) + \")\"\n",
    "    for y in range(2, n_months + 1):\n",
    "        tables2[\"temp\" + str(y)] = all_temps[y-1].columns\n",
    "        query = \" UNION ALL (SELECT loan_id, max_d12_gt3 + min_upb_12_eq0 as delinquency_12, upb_12, floor(((josh_mody_n * 12) + \" + str(\n",
    "            24000 + (y - 1)) + \")/12) as timestamp_year, josh_mody_n * 0 + \" + str(\n",
    "            y) + \" as timestamp_month from main.temp\" + str(y) + \")\"\n",
    "        union_query = union_query + query\n",
    "\n",
    "    results = pyblazing.run_query(union_query, tables2)\n",
    "    Chronometer.show(chronometer, 'Create 12 month features once')\n",
    "    return results\n",
    "\n",
    "\n",
    "def combine_joined_12_mon(joined_df, testdf, **kwargs):\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "    tables = {\"joined_df\": joined_df, \"testdf\": testdf}\n",
    "    query = \"\"\"SELECT j.loan_id, j.mrp_timestamp, j.timestamp_month, j.timestamp_year,\n",
    "                j.ever_30, j.ever_90, j.ever_180, j.delinquency_30, j.delinquency_90, j.delinquency_180,\n",
    "                t.delinquency_12, t.upb_12\n",
    "                FROM main.joined_df as j LEFT OUTER JOIN main.testdf as t\n",
    "                ON j.loan_id = t.loan_id and j.timestamp_year = t.timestamp_year and j.timestamp_month = t.timestamp_month\"\"\"\n",
    "    results = pyblazing.run_query(query, tables)\n",
    "    Chronometer.show(chronometer, 'Combine joind 12 month')\n",
    "    return results\n",
    "\n",
    "\n",
    "def final_performance_delinquency(gdf, joined_df, **kwargs):\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "    tables = {\"gdf\": gdf, \"joined_df\": joined_df}\n",
    "    query = \"\"\"SELECT g.loan_id, current_actual_upb, current_loan_delinquency_status, delinquency_12, interest_rate, loan_age, mod_flag, msa, non_interest_bearing_upb\n",
    "        FROM main.gdf as g LEFT OUTER JOIN main.joined_df as j\n",
    "        ON g.loan_id = j.loan_id and EXTRACT(YEAR FROM g.monthly_reporting_period) = j.timestamp_year and EXTRACT(MONTH FROM g.monthly_reporting_period) = j.timestamp_month \"\"\"\n",
    "    results = pyblazing.run_query(query, tables)\n",
    "    Chronometer.show(chronometer, 'Final performance delinquency')\n",
    "    return results\n",
    "\n",
    "\n",
    "def join_perf_acq_gdfs(perf, acq, **kwargs):\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "    tables = {\"perf\": perf, \"acq\": acq}\n",
    "    query = \"\"\"SELECT p.loan_id, current_actual_upb, current_loan_delinquency_status, delinquency_12, interest_rate, loan_age, mod_flag, msa, non_interest_bearing_upb,\n",
    "     borrower_credit_score, dti, first_home_buyer, loan_purpose, mortgage_insurance_percent, num_borrowers, num_units, occupancy_status,\n",
    "     orig_channel, orig_cltv, orig_date, orig_interest_rate, orig_loan_term, orig_ltv, orig_upb, product_type, property_state, property_type,\n",
    "     relocation_mortgage_indicator, seller_name, zip FROM main.perf as p LEFT OUTER JOIN main.acq as a ON p.loan_id = a.loan_id\"\"\"\n",
    "    results = pyblazing.run_query(query, tables)\n",
    "    Chronometer.show(chronometer, 'Join performance acquitistion gdfs')\n",
    "    return results\n",
    "\n",
    "\n",
    "def last_mile_cleaning(df, **kwargs):\n",
    "    chronometer = Chronometer.makeStarted()\n",
    "    print(df)\n",
    "    for col, dtype in df.dtypes.iteritems():\n",
    "        if str(dtype) == 'category':\n",
    "            df[col] = df[col].cat.codes\n",
    "        df[col] = df[col].astype('float32')\n",
    "    df['delinquency_12'] = df['delinquency_12'] > 0\n",
    "    df['delinquency_12'] = df['delinquency_12'].fillna(False).astype('int32')\n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].fillna(-1)\n",
    "    Chronometer.show(chronometer, 'Last mile cleaning')\n",
    "    return df\n",
    "\n",
    "\n",
    "use_registered_hdfs = False\n",
    "use_registered_posix = True\n",
    "\n",
    "if use_registered_hdfs:\n",
    "    register_hdfs()\n",
    "elif use_registered_posix:\n",
    "    register_posix()\n",
    "\n",
    "# to download data for this notebook, visit https://rapidsai.github.io/demos/datasets/mortgage-data and update the following paths accordingly\n",
    "\n",
    "acq_data_path = \"\"\n",
    "perf_data_path = \"\"\n",
    "col_names_path = \"\"\n",
    "if use_registered_hdfs:\n",
    "    acq_data_path = \"hdfs://myLocalHdfs/data/acq\"\n",
    "    perf_data_path = \"hdfs://myLocalHdfs/data/perf\"\n",
    "    col_names_path = \"hdfs://myLocalHdfs/data/names.csv\"\n",
    "elif use_registered_posix:\n",
    "    acq_data_path = \"/blazingdb/data/tpch/acq\"\n",
    "    perf_data_path = \"/blazingdb/data/tpch/perf\"\n",
    "    col_names_path = \"/blazingdb/data/tpch/names.csv\"\n",
    "\n",
    "start_year = 2000\n",
    "end_year = 2000  # end_year is inclusive\n",
    "start_quarter = 1\n",
    "end_quarter = 3\n",
    "#i think this can be commented\n",
    "#part_count = 1  # the number of data files to train against\n",
    "\n",
    "import time\n",
    "\n",
    "dxgb_gpu_params = {\n",
    "    'nround':            100,\n",
    "    'max_depth':         8,\n",
    "    'max_leaves':        2**8,\n",
    "    'alpha':             0.9,\n",
    "    'eta':               0.1,\n",
    "    'gamma':             0.1,\n",
    "    'learning_rate':     0.1,\n",
    "    'subsample':         1,\n",
    "    'reg_lambda':        1,\n",
    "    'scale_pos_weight':  2,\n",
    "    'min_child_weight':  30,\n",
    "    'tree_method':       'gpu_hist',\n",
    "    'n_gpus':            1,\n",
    "    'distributed_dask':  True,\n",
    "    'loss':              'ls',\n",
    "    'objective':         'reg:linear',\n",
    "    'max_features':      'auto',\n",
    "    'criterion':         'friedman_mse',\n",
    "    'grow_policy':       'lossguide',\n",
    "    # 'nthread', ncores[worker],  # WSM may want to set this\n",
    "    'verbose':           True\n",
    "}\n",
    "\n",
    "\n",
    "def range1(start, end):\n",
    "    return range(start, end+1)\n",
    "\n",
    "def use_file_type_suffix(year, quarter):\n",
    "    if year==2001 and quarter>=2:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def getChunks(year, quarter):\n",
    "    if use_file_type_suffix(year, quarter):\n",
    "        return range(0, 1+1)\n",
    "    return range(0, 0+1)\n",
    "\n",
    "def load_times(nro_nodes):\n",
    "    all_load_times = []\n",
    "    all_etl_times = []\n",
    "\n",
    "    for year in range1(start_year, end_year):\n",
    "        for quarter in range1(start_quarter, end_quarter):\n",
    "            with open('/blazingdb/data/tpch/results/'   +  str(year) + \"Q\" + str(quarter)  +'.txt', 'r') as file:\n",
    "                load_time, etl_time = [float(x) for x in next(file).split()] # read first line\n",
    "                all_load_times.append(load_time)\n",
    "                all_etl_times.append(etl_time)\n",
    "\n",
    "    #print(\"sum all_load_times:\", type(sum(all_load_times)))\n",
    "    #print(\"sum all_etl_times:\", type(sum(all_etl_times)))\n",
    "    #print(\"nro_nodes\", type(nro_nodes))\n",
    "    \n",
    "    print(\"TIMES SUMMARY Total Elapsed on all machines\")\n",
    "    print('LOAD Time: %fs' % ((sum(all_load_times))/nro_nodes))\n",
    "    print('ETL Time: %fs' % ((sum(all_etl_times))/nro_nodes))\n",
    "    #print('CONVERT Time: %fs' % sum(all_xgb_convert_times))\n",
    "\n",
    "    total_time_all_machines = (sum(all_load_times) + sum(all_etl_times)) / nro_nodes # + sum(all_xgb_convert_times)\n",
    "    percent_load = (sum(all_load_times) / total_time_all_machines) / nro_nodes\n",
    "    percent_etl = (sum(all_etl_times) / total_time_all_machines) / nro_nodes\n",
    "    #percent_xgb_convert = sum(all_xgb_convert_times) / total_time_all_machines\n",
    "\n",
    "    \"\"\"\n",
    "    print('Wall Time %fs' % (total_end - total_start) )\n",
    "    print('Wall LOAD Time: %fs' % ((total_end - total_start) * percent_load))\n",
    "    print('Wall ETL Time: %fs' % ((total_end - total_start) * percent_etl))\n",
    "    #print('Wall CONVERT Time: %fs' % (total_time_all_machines * percent_xgb_convert))\n",
    "    \"\"\"\n",
    "    \n",
    "    return (percent_load, percent_etl)\n",
    "\n",
    "final_cpu_df_label = None\n",
    "final_cpu_df_data = None\n",
    "\n",
    "all_load_times = []\n",
    "all_etl_times = []\n",
    "all_xgb_convert_times = []\n",
    "chunk_parameters = []\n",
    "\n",
    "client = Client('127.0.0.1:8786')\n",
    "#workers_ips = ['172.18.0.23', '172.18.0.24', '172.18.0.25']\n",
    "workers_ips = ['172.18.0.23', '172.18.0.24', '172.18.0.25', '172.18.0.26']\n",
    "\n",
    "total_start = time.time()\n",
    "for year in range1(start_year, end_year):\n",
    "    for quarter in range1(start_quarter, end_quarter):\n",
    "        for chunk in getChunks(year, quarter):\n",
    "            chunk_sufix = \"_{}\".format(chunk) if use_file_type_suffix(year, quarter) else \"\"\n",
    "            perf_file = perf_data_path + \"/Performance_\" + str(year) + \"Q\" + str(quarter) + \".txt\" + chunk_sufix\n",
    "            args = {\"quarter\" : quarter, \"year\" : year, \"perf_file\" : perf_file}\n",
    "            chunk_parameters.append(args)\n",
    "            \n",
    "futures = client.map(run_gpu_workflow, chunk_parameters, workers=workers_ips)\n",
    "\n",
    "client.gather(futures)\n",
    "#print(\"futures:\", futures)\n",
    "\n",
    "percent_load, percent_etl = load_times(len(workers_ips))\n",
    "\n",
    "total_end = time.time()\n",
    "\n",
    "#print(\"percents:\", percent_load, percent_etl, (total_end - total_start))\n",
    "\n",
    "print('Wall Time %fs' % (total_end - total_start) )\n",
    "print('Wall LOAD Time: %fs' % ((total_end - total_start) * percent_load))\n",
    "print('Wall ETL Time: %fs' % ((total_end - total_start) * percent_etl))\n",
    "#print('Wall CONVERT Time: %fs' % (total_time_all_machines * percent_xgb_convert))\n",
    "\n",
    "\n",
    "if use_registered_hdfs:\n",
    "    deregister_hdfs()\n",
    "elif use_registered_posix:\n",
    "    deregister_posix()\n",
    "\n",
    "\n",
    "Chronometer.show_resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
